# Collecting-audio-data-from-video-and-comparing-data-augmentation-methods-(Persian-audio-data)

## ğŸ“Œ Overview

This project explores how to collect and process voice segments from Persian-language films, and how different audio data augmentation methods impact the performance of emotion classification models.

Our goal is to test and compare augmentation techniques such as:
- Adding background noise
- Pitch shifting
- Time stretching
- Random mixed augmentation

We use Mel Spectrograms as the visual representation of audio, and train CNN-based models for emotion classification.

## ğŸ¯ Objectives

- Extract dialogue audio from Persian movie
- Split speech into segments using silence detection
- Manually label the emotional tone of segments into 4 classes:
  - 0: Neutral
  - 1: Happy
  - 2: Sad
  - 3: Angry
- Apply various audio augmentation methods
- Generate Mel Spectrograms from each version
- Train and evaluate CNN models (e.g., ResNet18)
- Visualize results using accuracy plots and confusion matrices


## ğŸ› ï¸ Installation

To run this project locally:

1. Clone the repository:
   ```bash
   git clone https://github.com/masume-r/Collecting-audio-data-from-video-and-comparing-data-augmentation-methods-Persian-audio-data-.git

cd comparing-data-augmentation-methods
pip install -r requirements.txt







## ğŸ—‚ï¸ Project Structure
comparing-data-augmentation-methods/ â”œâ”€â”€ notebooks/ # All Jupyter notebooks for each step â”‚ â”œâ”€â”€
 01_data_collection.ipynb â”‚ â”œâ”€â”€ 02_silence_segmentation.ipynb â”‚ â”œâ”€â”€ 03_create_csv_from_segments.ipynb â”‚ â”œâ”€â”€ 04_clean_and_balance_labeled_data.ipynb â”‚ â”œâ”€â”€ 05_split_final_data.ipynb â”‚ â”œâ”€â”€ 06_data_augmentation_final_fixed.ipynb â”‚ â”œâ”€â”€ 07_generate_melspectrogram.ipynb â”‚ â”œâ”€â”€ 08_prepare_augmented_csvs.ipynb â”‚ â”œâ”€â”€ 09_prepare_test_data.ipynb â”‚ â”œâ”€â”€ 10_Train_a_Model_(ResNet18_Fine_Tuning).ipynb â”‚ â”œâ”€â”€ 11_Train_a_Model_(ResNet18_Fine_Tuning)Noise_aug.ipynb â”‚ â”œâ”€â”€  12_Train_a_Model(ResNet18_Fine_Tuning)pitch_aug.ipynb â”‚ â”œâ”€â”€ 13_Train_a_Model(ResNet18_Fine_Tuning)stretched_aug.ipynb â”‚ â”œâ”€â”€ 14_Train_a_Model(ResNet18_Fine_Tuning)_Mixed_Augmentation.ipynb â”‚ â””â”€â”€ visualization.ipynb â”œâ”€â”€ requirements.txt # List of required Python packages â”œâ”€â”€ README.md # Project documentation (this file)


 
---

## ğŸ› ï¸ Main Dependencies

- Python 3.8+
- PyTorch
- torchvision
- torchaudio
- librosa
- matplotlib
- seaborn
- scikit-learn
- pandas
- numpy

All dependencies can be installed using:

```bash
pip install -r requirements.txt



## ğŸ“Š Results Overview

- Evaluated the effect of four different audio augmentation methods on emotion classification.
- Trained models using fine-tuned ResNet18 architecture.
- Visualized performance through confusion matrices and accuracy plots.
- Mixed augmentation showed slightly better generalization than individual methods.

evaluated each augmentation method by training a ResNet-18 model on the augmented datasets and measuring its best test accuracy. The comparison results are presented below.

### ğŸ“‹ Accuracy Table

![Accuracy Table](notebooks/table.png)

### ğŸ“ˆ Accuracy Comparison (Bar Chart)

This bar chart shows the best test accuracy achieved by each augmentation strategy:

![Accuracy Bar Chart](notebooks/plt.png)

### ğŸ“‰ Training Loss Over Epochs

This line chart visualizes how the training loss decreased over epochs for each version of the dataset:

![Training Loss](notebooks/plt-loss.png)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Contributing

Contributions are welcome!  
Feel free to open issues, create pull requests, or suggest improvements to make this project better.

---

## âœ¨ Acknowledgements

- Thanks to SeoulTech DS Lab for support and resources.
- Special thanks to the open-source community and libraries such as PyTorch, torchvision, torchaudio, librosa, and scikit-learn.







