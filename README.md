# Collecting-audio-data-from-video-and-comparing-data-augmentation-methods-(Persian-audio-data)

## ğŸ“Œ Overview

This project explores how to collect and process voice segments from Persian-language films, and how different audio data augmentation methods impact the performance of emotion classification models.

Our goal is to test and compare augmentation techniques such as:
- Adding background noise
- Pitch shifting
- Time stretching
- Random mixed augmentation

We use Mel Spectrograms as the visual representation of audio, and train CNN-based models for emotion classification.

## ğŸ¯ Objectives

- Extract dialogue audio from Persian movie
- Split speech into segments using silence detection
- Manually label the emotional tone of segments into 4 classes:
  - 0: Neutral
  - 1: Happy
  - 2: Sad
  - 3: Angry
- Apply various audio augmentation methods
- Generate Mel Spectrograms from each version
- Train and evaluate CNN models (e.g., ResNet18)
- Visualize results using accuracy plots and confusion matrices


## ğŸ› ï¸ Installation

To run this project locally:

1. Clone the repository:
   ```bash
   git clone https://github.com/masume-r/Collecting-audio-data-from-video-and-comparing-data-augmentation-methods-Persian-audio-data-.git
   cd comparing-data-augmentation-methods
   pip install -r requirements.txt


## ğŸ—‚ï¸ Project Structure
comparing-data-augmentation-methods/ â”œâ”€â”€ notebooks/ # All Jupyter notebooks for each step â”‚ â”œâ”€â”€
 01_data_collection.ipynb â”‚ â”œâ”€â”€ 02_silence_segmentation.ipynb â”‚ â”œâ”€â”€ 03_create_csv_from_segments.ipynb â”‚ â”œâ”€â”€ 04_clean_and_balance_labeled_data.ipynb â”‚ â”œâ”€â”€ 05_split_final_data.ipynb â”‚ â”œâ”€â”€ 06_data_augmentation_final_fixed.ipynb â”‚ â”œâ”€â”€ 07_generate_melspectrogram.ipynb â”‚ â”œâ”€â”€ 08_prepare_augmented_csvs.ipynb â”‚ â”œâ”€â”€ 09_prepare_test_data.ipynb â”‚ â”œâ”€â”€ 10_Train_a_Model_(ResNet18_Fine_Tuning).ipynb â”‚ â”œâ”€â”€ 11_Train_a_Model_(ResNet18_Fine_Tuning)Noise_aug.ipynb â”‚ â”œâ”€â”€  12_Train_a_Model(ResNet18_Fine_Tuning)pitch_aug.ipynb â”‚ â”œâ”€â”€ 13_Train_a_Model(ResNet18_Fine_Tuning)stretched_aug.ipynb â”‚ â”œâ”€â”€ 14_Train_a_Model(ResNet18_Fine_Tuning)_Mixed_Augmentation.ipynb â”‚ â””â”€â”€ visualization.ipynb â”œâ”€â”€ requirements.txt # List of required Python packages â”œâ”€â”€ README.md # Project documentation (this file)


 
---

## ğŸ› ï¸ Main Dependencies

- Python 3.8+
- PyTorch
- torchvision
- torchaudio
- librosa
- matplotlib
- seaborn
- scikit-learn
- pandas
- numpy




## ğŸ“Š Results Overview

- Evaluated the effect of four different audio augmentation methods on emotion classification.
- Trained models using fine-tuned ResNet18 architecture.
- Visualized performance through confusion matrices and accuracy plots.
- Mixed augmentation showed slightly better generalization than individual methods.

Evaluated each augmentation method by training a ResNet-18 model on the augmented datasets and measuring its best test accuracy. The comparison results are presented below.

### ğŸ“‹ Accuracy Table

![Accuracy Table](notebooks/table.png)

### ğŸ“ˆ Accuracy Comparison (Bar Chart)

This bar chart shows the best test accuracy achieved by each augmentation strategy:

![Accuracy Bar Chart](notebooks/plt.png)

### ğŸ“‰ Training Loss Over Epochs

This line chart visualizes how the training loss decreased over epochs for each version of the dataset:

![Training Loss](notebooks/plt-loss.png)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Contributing

Contributions are welcome!  
Feel free to open issues, create pull requests, or suggest improvements to make this project better. Although this project was conducted using Persian-language audio data, the entire pipeline is language-independent and can be applied to any language or speech dataset with minimal adjustments.

---

## âœ¨ Acknowledgements

- Special thanks to the open-source community and libraries such as PyTorch, torchvision, torchaudio, librosa, and scikit-learn.


## ğŸ“Œ ê¸°ìˆ  í‰ê°€ í•­ëª© ëŒ€ì‘ ë‚´ì—­

- **[1. ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ ê³µí†µ]**  
  Pythonì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬, ì˜¤ë””ì˜¤ ë¶„í• , ë¼ë²¨ë§, ëª¨ë¸ í•™ìŠµ ë“± ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

- **[3. ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ]**  
  ì˜í™”ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³ , ë¬´ìŒ ê°ì§€ë¥¼ í†µí•´ ë°œí™” ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.  
  .wav ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³ , Mel Spectrogramìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

- **[4. ìˆ˜ì§‘í•œ ë°ì´í„°ì…‹ì˜ ìœ ìš©ì„±]**  
  ì§ì ‘ ê°ì • ë¼ë²¨ë§ì„ ìˆ˜í–‰í•˜ì—¬ 4ê°€ì§€ ê°ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ì˜€ê³ ,  
  ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ ë° ì •ì œí•˜ì—¬ ëª¨ë¸ í›ˆë ¨ì— í™œìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

- **[7. ë°ì´í„° í™œìš© ë° ë¶„ì„]**  
  PyTorchì™€ torchvisionì„ ì‚¬ìš©í•˜ì—¬ Mel Spectrogram ê¸°ë°˜ ResNet18 ëª¨ë¸ì„ í•™ìŠµí•˜ì˜€ê³ ,  
  ê°ì • ë¶„ë¥˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ 4ê°€ì§€ ì˜¤ë””ì˜¤ ë°ì´í„° ì¦ê°• ê¸°ë²•  
  (**ë°°ê²½ ì†ŒìŒ ì¶”ê°€, í”¼ì¹˜ ë³€ì¡°, ì†ë„ ì¡°ì ˆ, í˜¼í•© ì¦ê°•**)ì„ ì ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

- **[8. ë°ì´í„° ì‹œê°í™”]**  
  ì¦ê°• ê¸°ë²•ë³„ ì •í™•ë„ ë¹„êµ, í˜¼ë™ í–‰ë ¬, í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„ ë“±ì„ ì‹œê°í™”í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.




