# Collecting-audio-data-from-video-and-comparing-data-augmentation-methods-(Persian-audio-data)

## ğŸ“Œ Overview

This project explores how to collect and process voice segments from Persian-language films, and how different audio data augmentation methods impact the performance of emotion classification models.

Our goal is to test and compare augmentation techniques such as:
- Adding background noise
- Pitch shifting
- Time stretching
- Random mixed augmentation

We use Mel Spectrograms as the visual representation of audio, and train CNN-based models for emotion classification.

## ğŸ¯ Objectives

- Extract dialogue audio from Persian movie
- Split speech into segments using silence detection
- Manually label the emotional tone of segments into 4 classes:
  - 0: Neutral
  - 1: Happy
  - 2: Sad
  - 3: Angry
- Apply various audio augmentation methods
- Generate Mel Spectrograms from each version
- Train and evaluate CNN models (e.g., ResNet18)
- Visualize results using accuracy plots and confusion matrices


## ğŸ—‚ï¸ Project Structure

fa-audio-augmentation/ â”œâ”€â”€ README.md â”œâ”€â”€ requirements.txt â”œâ”€â”€ notebooks/ â”‚ â”œâ”€â”€ 01_data_collection.ipynb â”‚ â”œâ”€â”€ 02_silence_segmentation.ipynb â”‚ â”œâ”€â”€ 03_augmentation_noise.ipynb â”‚ â”œâ”€â”€ 04_augmentation_pitch.ipynb â”‚ â”œâ”€â”€ 05_augmentation_stretch.ipynb â”‚ â”œâ”€â”€ 06_augmentation_mixed.ipynb â”‚ â”œâ”€â”€ 07_melspectrogram_generation.ipynb â”‚ â””â”€â”€ 08_model_training.ipynb â”œâ”€â”€ results/ â”‚ â”œâ”€â”€ accuracy_plots/ â”‚ â”œâ”€â”€ confusion_matrices/ â”‚ â””â”€â”€ summary_metrics.csv â”œâ”€â”€ models/ â”‚ â””â”€â”€ best_model_noise.pth â”œâ”€â”€ utils/ â”‚ â”œâ”€â”€ audio_utils.py â”‚ â””â”€â”€ plot_utils.py â””â”€â”€ .gitignore
